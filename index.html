<!DOCTYPE html>
<html>

<head>
	<title>IEEE JSTSP 2025</title>
	<meta charset="utf-8" />
	<link href="static/css/bootstrap.min.css" rel="stylesheet" type="text/css" />
	<link href="static/css/main.css" media="screen,projection" rel="stylesheet" type="text/css" />
</head>

<body>
	<div class="navbar navbar-default navbar-fixed-top">
		<div class="container">
			<div class="navbar-header"><button class="navbar-toggle" data-target="#navbar-main" data-toggle="collapse"
					type="submit"></button></div>
			<div class="navbar-collapse collapse" id="navbar-main">
				<ul class="nav navbar-nav">
					<li><a href="#scope">Scope</a></li>
					<li><a href="#topics">Topics</a></li>
					<li><a href="#important_Dates">Important Dates</a></li>
					<li><a href="#guest_editors">Guest Editors</a></li>
				</ul>
			</div>
		</div>
	</div>

	<div class="container">
		<div class="page-content">
			<p>&nbsp;</p>

			<div class="row" id="dates">
				<div class="col-xs-12">
					<center>
						<h1><b>IEEE JSTSP Special Issue on </b>Deep Multimodal Speech Enhancement and Separation</h1>
					</center>

					<h2>Call for Papers</h2>
					<h3 style="margin-top: 20px;">Manuscript Due: 30 September 2024</h3>
					<h3>Publication Date: May 2025</h3>
					<h4 style="margin-top: 20px;">The IEEE Journal of Selected Topics in Signal Processing invites
						submissions for a Special Issue on Deep Multimodal Speech Enhancement and Separation</h4>

					<!--Scope-->
					<div class="row" id="scope" style="margin-top: 30px;">
						<div class="col-xs-12">
							<h2>Scope</h2>
						</div>
					</div>
					<div style="font-size: 16px;text-align:justify">Voice is the most commonly used modality
						by humans to communicate and psychologically blend into society. Recent technological advances
						have triggered the development of various voice-related applications in the information and
						communications technology market. However, noise, reverberation, and interfering speech are
						detrimental for effective communications between humans and other humans or machines, leading to
						performance degradation of associated voice-enabled services. To address the formidable
						speech-in-noise challenge, a range of speech enhancement (SE) and speech separation (SS)
						techniques are normally employed as important front-end speech processing units to handle
						distortions in input signals in order to provide more intelligible speech for automatic speech
						recognition (ASR), synthesis and dialogue systems. Emerging advances in artificial intelligence
						(AI) and machine learning, particularly deep neural networks, have led to remarkable
						improvements in SE and SS based solutions. A growing number of researchers have explored various
						extensions of these methods by utilising a variety of modalities as auxiliary inputs to the main
						speech processing task to access additional information from heterogeneous signals. In
						particular, multi-modal SE and SS systems have been shown to deliver enhanced performance in
						challenging noisy environments by augmenting the conventional speech modality with complementary
						information from multi-sensory inputs, such as video, noise type, signal-to-noise ratio (SNR),
						bone-conducted speech (vibrations), speaker, text information, electromyography, and
						electromagnetic midsagittal articulometer (EMMA) data. Various integration schemes, including
						early and late fusions, cross-attention mechanisms, and self-supervised learning algorithms,
						have also been successfully explored. </div>

					<hr />

					<!--Topic-->
					<div class="row" id="topics" style="margin-top: 30px;">
						<div class="col-xs-12">
							<h2>Topics</h2>
						</div>
					</div>
					<div style="font-size: 16px;text-align:justify ;margin-top: 10px;">This timely special
						issue aims to collate latest advances in multi-modal SE and SS systems that exploit both
						conventional and unconventional modalities to further improve state-of-the-art performance in
						benchmark problems. We particularly welcome submissions for novel deep neural network based
						algorithms and architectures, including new feature processing methods for multimodal and
						cross-modal speech processing. We also encourage submissions that address practical issues
						related to multimodal data recording, energy-efficient system design and real-time low-latency
						solutions, such as for assistive hearing and speech communication applications.
					</div>

					<div class="row" id="topics1" style="margin-top: 10px;">
						<div class="col-xs-12">

							<h2><span style="font-size: 16px">Special Issue research topics of
									interest relate to open problems needing addressed. These include, but are not
									limited to, the following.</span></h2>
						</div>
					</div>

					<div class="row">
						<div class="col-xs-12">
							<ul>
								<li><span style="font-size: 16px">Novel acoustic features and
										architectures for multi-modal SE (MM-SE) and multi-modal SS (MM-SS)
										solutions.</span></li>
								<li><span style="font-size: 16px">The integration of
										multiple data acquisition devices for multimodal learning and novel learning
										algorithms robust to imperfect data.</span></li>
								<li><span style="font-size: 16px">Few-shot/zero-shot learning and adaptation
										algorithms for MM-SE and MM-SS systems with a small amount of training and
										adaptation data.</span></li>
								<li><span style="font-size: 16px">Self-supervised and unsupervised learning
										techniques for MM-SE and MM-SS systems. </span></li>
								<li><span style="font-size: 16px">Adversarial learning for MM-SE and
										MM-SS.</span></li>
								<li><span style="font-size: 16px">Large language model-based Generative
										approaches for MM-SE and MM-SS&nbsp;</span></li>
								<li><span style="font-size: 16px">Low-delay, low-power, low-complexity
										MM-SE and MM-SS models&nbsp;</span></li>
								<li><span style="font-size: 16px">Approaches that effectively reduce
										model size and inference cost without reducing the speech quality and
										intelligibility of processed signals.
										&nbsp;</span></li>
								<li><span style="font-size: 16px">Novel objective functions including
										psychoacoustics and perceptually motivated loss functions for MM-SE and
										MM-ES &nbsp;</span></li>
								<li><span style="font-size: 16px">Holistic evaluation metrics for MM-SE
										and MM-SS systems. &nbsp;</span></li>
								<li><span style="font-size: 16px">Real-world applications and use-cases
										of MM-SE and MM-SS, including human-human and human-machine communications
										&nbsp;</span></li>
								<li><span style="font-size: 16px">Challenges and solutions in the
										integration of MM-SE and MM-SS into existing systems
										&nbsp;</span></li>
							</ul>
						</div>
					</div>
					<div style="font-size: 16px;text-align:justify ;margin-top: 10px;">We encourage
						submissions that not only propose novel approaches but also substantiate the findings with
						rigorous evaluations, including real-world datasets. Studies that provide insights into the
						challenges involved and the impact of MM-SE and MM-SS on end-users are particularly welcome.
					</div>
					<div style="font-size: 16px;text-align:justify ;margin-top: 10px;"><b>Submission
							Guidelines: </b>Manuscripts should be original and should not have been previously
						published or currently under consideration for publication elsewhere. All submissions will be
						peer-reviewed according to the IEEE Signal Processing Society review process. Authors should
						prepare their manuscripts according to the Instructions for Authors available from the
						Signal Processing Society website.
					</div>
					<div style="font-size: 16px;text-align:justify ;margin-top: 10px;">Follow the
						instructions given on the IEEE <a class="blue-text"
							href="https://signalprocessingsociety.org/publications-resources/ieee-journal-selected-topicssignal-processing">JSTSP
							webpage</a> and <a class="blue-text"
							href="https://mc.manuscriptcentral.com/jstsp-ieee">submit
							manuscripts</a>.

					</div>

					<hr />

					<!--Important Dates-->
					<div class="row" id="important_Dates" style="margin-top: 30px;">
						<div class="col-xs-12">
							<h2>Important Dates</h2>
						</div>
					</div>
					<div class="table-container">
						<table>
							<tr>
								<td>Submissions due:</td>
								<td style="padding: 0 20px;">30 September 2024</td>
							</tr>
							<tr>
								<td>First Review due:</td>
								<td style="padding: 0 20px;">15 December 2024</td>
							</tr>
							<tr>
								<td>Revised manuscript due:</td>
								<td style="padding: 0 20px;">15 January 2025</td>
							</tr>
							<tr>
								<td>Second review due:</td>
								<td style="padding: 0 20px;">15 February 2025</td>
							</tr>
							<tr>
								<td><b>Final Decision:</b></td>
								<td style="padding: 0 20px;"><b>28 February 2025</b></td>
							</tr>
						</table>
					</div>

					<hr />

					<!--Guest Editors-->
					<div class="row" id="guest_editors" style="margin-top: 30px;">
						<div class="col-xs-12">
							<h2>Guest Editors</h2>
						</div>
					</div>

					<div style="font-size: 16px;text-align:justify ;margin-top: 10px;">
						For further information, please contact the guest editors at:
					</div>
					<div class="table-container">
						<table>
							<tr>
								<td style="padding: 5px;">AMIR HUSSAIN (Edinburgh Napier University, UK,
									hussain.doctor@gmail.com) (Lead GE)
								</td>
							</tr>
							<tr>
								<td style="padding: 5px;">YU TSAO (Academia Sinica, Taiwan, yu.tsao@citi.sinica.edu.tw)
									(co-Lead GE)</td>
							</tr>
							<tr>
								<td style="padding: 5px;">JOHN H.L. HANSEN (University of Texas at Dallas, USA,
									john.hansen@utdallas.edu)</td>
							</tr>
							<tr>
								<td style="padding: 5px;">NAOMI HARTE (Trinity College Dublin, Ireland, NHARTE@tcd.ie)
								</td>
							</tr>
							<tr>
								<td style="padding: 5px;">SHINJI WATANABE (Carnegie Mellon University, USA,
									swatanab@andrew.cmu.edu)</td>
							</tr>
							<tr>
								<td style="padding: 5px;">ISABEL TRANCOSO (Instituto Superior T&eacute;cnico, IST,
									Univ. Lisbon, Portugal,
									sabel.trancoso00@gmail.com)</td>
							</tr>
							<tr>
								<td style="padding: 5px;">SHIXIONG ZHANG (Tencent AI Lab, USA,
									auszhang@global.tencent.com)</td>
							</tr>
						</table>
					</div>

					<div style="font-size: 16px;text-align:justify;margin-top: 10px;" class="red-text">
						Dear Editor and Reviewers:
					</div>
					<div style="font-size: 16px;text-align:justify;margin-top: 10px;">
						<span class="red-text">Special Issue Proposal for the IEEE JSTSP Title:</span> Deep Multimodal
						Speech Enhancement and Separation
					</div>

					<div style="font-size: 16px;text-align:justify;margin-top: 10px;" class="red-text">
						The Guest Editors very much appreciate the constructive comments and suggestions provided by the
						Editor and Reviewers. The attached special issue proposal has been substantially revised in
						light of these.
					</div>

					<div style="font-size: 16px;text-align:justify;margin-top: 20px;">
						* Timeliness:
					</div>
					<div style="font-size: 16px;text-align:justify;">
						"The description does not highlight clear new contributions. For example, the first paper listed
						under timeliness from 2017 has only one citation."
					</div>
					<div style="font-size: 16px;text-align:justify;" class="red-text">
						Response: References have been updated.
					</div>

					<div style="font-size: 16px;text-align:justify;margin-top: 20px;">
						* Broadness:
					</div>
					<div style="font-size: 16px;text-align:justify;">
						"Not so many groups working in this area. The list of potential authors is not so long."
					</div>

					<div style="font-size: 16px;text-align:justify;margin-top: 20px;" class="red-text">
						Response: The list of potential authors has been expanded to demonstrate growing interest in
						this area.
					</div>

					<div style="font-size: 16px;text-align:justify;margin-top: 20px;">
						* Cross-communities:
					</div>
					<div style="font-size: 16px;text-align:justify;">
						"No obvious relation to other societies and also none listed in the proposal."
					</div>
					<div style="font-size: 16px;text-align:justify;">
						"Not so relevant for other IEEE societies."
					</div>
					<div style="font-size: 16px;text-align:justify;" class="red-text">
						Response: Relevance to other IEEE societies has been clarified.
					</div>

					<div style="font-size: 16px;text-align:justify;margin-top: 20px;">
						* Target Authors:
					</div>
					<div style="font-size: 16px;text-align:justify;">
						"Reasonable list but somewhat short."
					</div>
					<div style="font-size: 16px;text-align:justify;">
						"Good paper list; could have had more authors."
					</div>
					<div style="font-size: 16px;text-align:justify;" class="red-text">
						Response: The list of potential authors had been expanded.
					</div>

					<div style="font-size: 16px;text-align:justify;margin-top: 20px;">
						* Guest editors' diversity:
					</div>
					<div style="font-size: 16px;text-align:justify;">
						"This is a weak area of the proposal. There is no GE from industry."
					</div>
					<div style="font-size: 16px;text-align:justify;">
						"Good geographical and topic diversity. Poor gender diversity."
					</div>
					<div style="font-size: 16px;text-align:justify;" class="red-text">
						Response: There are now two females in our guest editor team.
						<span class="purple-text">One guest editor is from the industry.</span>
					</div>

					<div style="font-size: 16px;text-align:justify;margin-top: 20px;" class="red-text">
						<b>** Suggested improvements:</b>
					</div>
					<div style="font-size: 16px;text-align:justify;">
						While an important component: Drafted CFP is missing in the present version of the proposal, I
						still believe that this is worthy to be accepted in terms of all the other required aspects for
						a special issue proposal.
					</div>
					<div style="font-size: 16px;text-align:justify;" class="red-text">
						Response: A draft CFP is now appended at the end of the proposal.
					</div>

					<div style="font-size: 16px;text-align:justify;margin-top: 20px;">
						One minor note: the last guest editor serves also another special issue of this journal about
						the similar timeline and I am not sure if there is any workload/conflict issue. (EIC note: This
						is an undesired situation.)
					</div>
					<div style="font-size: 16px;text-align:justify;" class="red-text">
						Response: The last guest editor has been replaced with Prof Isabel Trancoso, which also
						strengthens the gender diversity of our team.
					</div>

					<div style="font-size: 16px;text-align:justify;margin-top: 20px;">
						Improve industry component in the GE team.
					</div>
					<div style="font-size: 16px;text-align:justify;">
						<span class="red-text">Response:</span> Dr. Shixiong Zhang from Tencent AI Lab has joined our
						guest editorial team.
					</div>

					<div style="font-size: 16px;text-align:justify;margin-top: 20px;">
						1) multimodal: the authors are encouraged to elaborate more on "multimodal" for SS/SE, e.g.,
						which multimodal signals are beneficial and how these signals help SS and SE. Multimodal seems a
						highlighted difference, but the description for now is not that clear.
					</div>
					<div style="font-size: 16px;text-align:justify;">
						<span class="red-text">Response:</span> We have tried to address this issue in further detail.
					</div>

					<div style="font-size: 16px;text-align:justify;margin-top: 20px;">
						2) the title of this SI might be optimized (e.g., Deep Multimodal Speech Enhancement and
						Separation).
					</div>
					<div style="font-size: 16px;text-align:justify;" class="red-text">
						Response: Done
					</div>

					<div style="font-size: 16px;text-align:justify;margin-top: 20px;" class="red-text">
						Guest Editor team:
					</div>
					<div style="font-size: 16px;text-align:justify;" class=" red-text">
						Profs. Amir Hussain, Yu Tsao, John H. L. Hansen, Namoi Harte, Shinji Watanabe, Isabel Trancoso
						and Shixiong Zhang
					</div>

					<div style="font-size: 16px;text-align:justify;margin-top: 30px;">
						<b>* Special Issue Proposal for the IEEE JSTSP</b>
					</div>
					<div style="font-size: 16px;text-align:justify;margin-top: 10px;">
						Deep Multimodal Speech Enhancement and Separation
					</div>
					<div style="font-size: 16px;text-align:justify;margin-top: 20px;">
						A. Abstract and Relevance
					</div>
					<div style="font-size: 16px;text-align:justify; margin-top: 10px;">
						Voice is the most commonly used modality by humans to communicate and psychologically blend into
						society. Recent technological advances have triggered the development of various voice-related
						applications in the information and communications technology market. However, noise,
						reverberation, and interfering speech are detrimental for effective communications between
						humans and other humans or machines, leading to performance degradation of associated
						voice-enabled services. To address the formidable speech-in-noise challenge, a range of speech
						enhancement (SE) and speech separation (SS) techniques are normally employed as important
						front-end speech processing units to handle distortions in input signals in order to provide
						more intelligible speech for automatic speech recognition (ASR), synthesis and dialogue systems.
					</div>
					<div style="font-size: 16px;text-align:justify;margin-top: 10px;">
						Emerging advances in artificial intelligence (AI) and machine learning, particularly deep neural
						networks, have led to remarkable improvements in SE and SS based solutions. A growing number of
						researchers have explored various extensions of these methods by utilising a variety of
						modalities as auxiliary inputs to the main speech processing task to access additional
						information from heterogeneous signals. In particular, multi-modal SE and SS systems have been
						shown to deliver enhanced performance in challenging noisy environments by augmenting the
						conventional speech modality with complementary information from multi-sensory inputs, such as
						video<span class=" red-text">(e.g for tracking lip movements)</span>, noise type,
						signal-to-noise ratio (SNR), bone-conducted speech (vibrations), speaker, text information,
						electromyography, and electromagnetic midsagittal articulometer (EMMA) data. Various integration
						schemes, including early and late fusions, cross-attention mechanisms, and self-supervised
						learning algorithms, have also been successfully explored.
					</div>
					<div style="font-size: 16px;text-align:justify;margin-top: 10px;">
						This timely special issue aims to collate latest advances in multi-modal SE and SS systems that
						exploit both conventional and unconventional modalities to further improve state-of-the-art
						performance in benchmark problems. We particularly welcome submissions for novel deep neural
						network based algorithms and architectures, including new feature processing methods for
						multimodal and cross-modal speech processing. We also encourage submissions that address
						practical issues related to multimodal data recording, energy-efficient system design and
						real-time low-latency solutions, such as for assistive hearing and speech communication
						applications.
					</div>
					<div style="font-size: 16px;text-align:justify;margin-top: 10px;">
						We anticipate that this special issue can make a valuable contribution to integration with
						various subsequent tasks, including
					</div>
					<div style="font-size: 16px;text-align:justify; margin-top: 10px;" class=" red-text">
						<ul style="list-style-type: none;">
							<li><span style="font-size: 16px">(1) Biomedical engineering: assistive hearing
									technologies
								</span></li>
							<li><span style="font-size: 16px">(2) Affective computing:
									multimodal emotion/pathological recognition
								</span></li>
							<li><span style="font-size: 16px">(3) Human-computer interface: multimodal ASR in
									noise/reverberant conditions
								</span></li>
							<li><span style="font-size: 16px" class=" red-text">AR/VR: enhanced speech can provide
									better quality to AR/VR applications
								</span></li>
						</ul>
					</div>

					<div style="font-size: 16px;text-align:justify;" class=" red-text">
						Outcomes of this special issue will be of relevance to the scope of various other IEEE
						societies, such as:
					</div>
					<div style="font-size: 16px;text-align:justify;margin-top: 10px;" class=" red-text">
						<ul style="list-style-type: none;">
							<li><span style="font-size: 16px">(1) IEEE Consumer Technology Society (CTSoc):
									The subject matter has direct relevance to practical applications in consumer
									electronics
								</span></li>
							<li><span style="font-size: 16px">(2) IEEE Engineering in Medicine
									and Biology Society (EMBS): Applications of special issue research topics in
									assistive hearing and speech technologies will advance the interdisciplinary field
									of biomedical engineering.
								</span></li>
							<li><span style="font-size: 16px">(3) IEEE Society for Social Implications of Technology
									(SSIT): Multi-modal assistive hearing and speech communication devices targetted by
									this special issue can deliver significant contributions to society.
								</span></li>
							<li><span style="font-size: 16px">(4) IEEE Circuits and Systems Society (CSS): The special
									issue features novel model architectures to implement multimodal speech enhancement
									in real-world applications, considering practical computational, energy and latency
									constraints. This aspect holds significance relevance for IEEE CSS.
								</span></li>
						</ul>
					</div>

					<div style="font-size: 16px;text-align:justify;">
						Special Issue research topics of interest relate to open problems needing addressed. These
						include, but are not limited to, the following.
					</div>
					<div class="row">
						<div class="col-xs-12">
							<ul>
								<li><span style="font-size: 16px">Novel acoustic features and architectures
										for multi-modal SE (MM-SE) and multi-modal SS (MM-SS) solutions. </span></li>
								<li><span style="font-size: 16px">The integration of multiple
										data acquisition devices for multimodal learning and novel learning algorithms
										robust to imperfect data.</span></li>
								<li><span style="font-size: 16px">Few-shot/zero-shot learning and adaptation algorithms
										for MM-SE and MM-SS systems with a small amount of training and adaptation
										data.</span></li>
								<li><span style="font-size: 16px">Self-supervised and unsupervised learning techniques
										for MM-SE and MM-SS systems.
									</span></li>
								<li><span style="font-size: 16px">Approaches that effectively reduce model
										size and inference cost without reducing the speech quality and intelligibility
										of processed signals.
									</span></li>
								<li><span style="font-size: 16px">Novel objective functions that
										specifically aim to improve speech intelligibility/quality/automatic speech
										recognition accuracy.
										&nbsp;</span></li>
								<li><span style="font-size: 16px">Novel applications of MM-SE and MM-SS in
										human-human and human-machine communications.
										&nbsp;</span></li>
								<li><span style="font-size: 16px">Holistic evaluation metrics for MM-SE and
										MM-SS systems.
										&nbsp;</span></li>
							</ul>
						</div>
					</div>

					<div style="font-size: 16px;text-align:justify;margin-top: 10px;">
						B. Timeliness
					</div>
					<div style="font-size: 16px;text-align:justify; margin-top: 10px;">
						In recent years, a growing number of researchers have attempted to incorporate diverse
						modalities as auxiliary inputs for SE and SS models to access additional information. Visual
						clues represent an important modality that carry information complementary to speech signals for
						everyday communication. For example, the McGurk effect refers to cross-effects between
						visualized mouth or lip shapes and human hearing perception. From this perspective, numerous
						audio-visual MM-SE and MM-SS approaches have been proposed.
					</div>
					<div style="font-size: 16px;text-align:justify;margin-top: 10px;">
						Related work by ASR researchers has also demonstrated the potential of audio-visual speech
						recognition to improve the noise robustness of speech recognition in the real world (e.g.
						<a class="blue-text"
							href="https://doi.org/10.20965/jrm.2017.p0105">https://doi.org/10.1109/TPAMI.2018.2889052)</a>.
						Such multimodal approaches clearly demonstrate that
						visual cues can successfully enhance the performance of audio-only speech processing. In
						addition to visual information, several studies have proposed contextually incorporating speaker
						and speaking environment modules into SE and SS models. For example, speaking environment
						information such as SNR and noise types has been used to enhance SE models. Other studies have
						used speaker identity as prior knowledge for implementing SE or SS systems.
					</div>
					<div style="font-size: 16px;text-align:justify;margin-top: 10px;">
						A recent timely review of audio-visual based SE and SS approaches was published in the IEEE
						TASLP, which reported a significant and growing number of influential works(
						<a class="blue-text"
							href="https://ieeexplore.ieee.org/document/9380418">https://ieeexplore.ieee.org/document/9380418</a>).
						Other notable examples of ongoing globally
						impactful interdisciplinary research in this area include the UK Engineering and Physical
						Sciences Research Council (EPSRC) funded multi-million pound research programme (COG-MHEAR) that
						is developing real-time cognitively-inspired MM-SE and MM-SS approaches to transform the
						next-generation of assistive hearing and speech communication technology (<a class="blue-text"
							href="http://cogmhear.org">http://cogmhear.org</a>).

					</div>

					<div style="font-size: 16px;text-align:justify ;margin-top: 10px;">A related 2023 ICASSP
						Satellite Workshop was recently successfully organised on &quot;Advances in multi-modal hearing
						assistive technologies (AMHAT)&quot;. This complements the world&apos;s first annual
						Audio-Visual Speech
						Enhancement Challenge (AVSEC) organised as part of the 2023 IEEE ASRU Workshop(
						<a class="blue-text" href="http://challenge.cogmhear.org">http://challenge.cogmhear.org</a>),
						and the Speech Enhancement for Augmented Reality (SPEAR) Challenge (<a class="blue-text"
							href="https://imperialcollegelondon.github.io/spear-challenge/">https://imperialcollegelondon.github.io/spear-challenge/</a>).
					</div>

					<div style="font-size: 16px;text-align:justify; margin-top: 20px;">
						C. Applications
					</div>
					<div class="row" style="margin-top: 10px;">
						<div class="col-xs-12">
							<ul>
								<li><span style="font-size: 16px">The MM-SE and MM-SS topics covered in
										this proposed special issue span a wide range of challenging real-world
										applications, including the following.</span></li>
								<li><span style="font-size: 16px">Important downstream tasks:
										automatic speech recognition, speaker and language recognition, voice
										conversion, and speech synthesis.
									</span></li>
								<li><span style="font-size: 16px">Assistive listening, visual and multimodal speech
										communication technologies.</span></li>
								<li><span style="font-size: 16px">Multimedia processing: multi-modal sentiment analysis,
										multi-modal dialog systems, multi-modal information retrieval.
									</span></li>
								<li><span style="font-size: 16px">Cognitive robotics, including more
										natural, multi-modal human-robot interaction and emerging AR/VR applications
									</span></li>
								<li><span style="font-size: 16px">Wearable devices such as smart glasses
										and chatbots as multimodal communication aids
										&nbsp;</span></li>
							</ul>
						</div>
					</div>

					<div style="font-size: 16px;text-align:justify;margin-top: 20px;">
						D. Broad Impact
					</div>
					<div style="font-size: 16px;text-align:justify; margin-top: 10px;">
						With recent advances in sensing, computing, and communication technologies, vast amounts of
						audio, text, and video data can be accessed easily. Significant efforts have been made to
						combine complementary information from multiple data sources to facilitate improved speech
						signal processing performance. Notable achievements have been demonstrated for multimodal speech
						processing systems compared to mono-modal speech processing. This special issue focuses on
						advanced artificial intelligence models and methods for speech enhancement and separation with
						heterogeneous data, which we believe will elicit broad interest from various research
						communities, including in data acquisition, text processing, computer vision, speech, and
						multimedia processing. In particular, the unique focus of this special issue on cross-modality
						and multi-modality SE and SS approaches will impact a range of interdisciplinary research areas
						across speech, hearing and language processing, including for robotics, wearable devices, more
						natural human-computer interaction and emerging AR/VR applications.
					</div>

					<div style="font-size: 16px;text-align:justify;margin-top: 20px;">
						E. Broad Impact
					</div>
					<div style="font-size: 16px;text-align:justify; margin-top: 10px;">
						The proposed special issue spans several areas of the IEEE SIGNAL PROCESSING SOCIETY. We aim to
						elicit submissions from multidisciplinary fields, specifically data acquisition, text
						processing, computer vision, speech, machine learning, model compression, and multimedia
						processing.
					</div>

					<div style="font-size: 16px;text-align:justify;margin-top: 20px;">
						F. Redundancy
					</div>
					<div style="font-size: 16px;text-align:justify; margin-top: 10px;">
						We believe our proposal is the first special issue dedicated to advancing research in MM-SE and
						MM-SS to address a range of real-world speech-in-noise challenges. This is evidenced by our
						review of the following related special issues:
					</div>
					<div style="font-size: 16px;text-align:justify; margin-top: 10px;">
						The 2019 <i>IEEE JSTSP</i> Special issue on Far-Field Speech Processing in the Era of Deep
						Learning" in 2019 dealt with speech enhancement and separation but did not include multimodal
						aspects. Similarly the 2020 IEEE JSTSP Special Issue on "Reconstruction of audio from incomplete
						or highly degraded observations" also dealt with speech enhancement but did not address
						multimodal aspects. More recently, the 2022 IEEE JSTSP Special Issue on "Self-Supervised
						Learning for Speech and Audio Processing" dealt with general speech and audio processing
						applications but did not address multi-modal speech enhancement and separation challenges.
					</div>

					<div style="font-size: 16px;text-align:justify;margin-top: 20px;">
						G. Potential contributors
					</div>
					<div style="font-size: 16px;text-align:justify; margin-top: 10px;">
						Researchers expected to contribute original and tutorial/review papers to the SI are listed
						below:
					</div>
					<div style="font-size: 16px;text-align:justify;margin-top: 10px;">
						<ul style="list-style-type: none;">
							<li><span style="font-size: 16px">Prof Jesper Jensen, Prof Zheng-Hua Tan,
									Aalborg University, Denmark
								</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px">Prof Nima Mesgarani, Columbia
									University, USA
								</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px">Prof Qiang Huang, University of
									Sunderland, UK
								</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px">Prof Bernd T. Meyer, University
									of Oldenburgh, Germany
								</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px">Dr Daniel Michelsanti, Oticon,
									Denmark
								</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px">Prof Dong Yu, Prof Meng Yu and
									Prof Yong Xu, Tencent AI Lab, USA</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px">Prof Sabato Marco Siniscalchi,
									Norwegian University of Science and Technology</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px">Prof Jun Du, University of
									Science and Technology of China (USTC), China</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px">Dr Erfan Loweimi, University of
									Cambridge, UK
								</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px">Prof Ben Milner, University of
									East Anglia, UK
								</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px">Prof Jun-Cheng Chen, Academia
									Sinica, Taiwan
								</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px">Prof Isabel Trancoso, IST, Univ.
									Lisbon, Portugal
								</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px">Professor Xiao-Lei Zhang,
									Northwestern Polytechnical University, China</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px">Professor Chin-Hui Lee, Georgia
									Institute of Technology,USA</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px">Dr Andrew Abel, University of
									Strathclyde, Glasgow, UK</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px">Dr Kia Dashtipour, Dr Tassadaq
									Hussain, Edinburgh Napier University, UK</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px" class="red-text">Prof. Haizhou
									Li, The Chinese University of Hong Kong, China</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px" class="red-text">Dr. Gordon
									Wichern, Fran&ccedil;ois G Germain, Sameer Khurana, Chiori Hori, Jonathan LeRoux,
									Mitsubishi Electric Research
									Laboratories (MERL)</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px" class="red-text">Dr. Yoshiki
									Masuyama, Tokyo Metropolitan University</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px" class="red-text">Prof. Maja
									Pantic, Imperial College London, UK</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px" class="red-text">Prof. Ahsan
									Adeel, University of Stirling, UK</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px" class="red-text">Prof. Qingyao
									Wu, South China Universityof Technology, China</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px" class="red-text">Prof. Xinman
									Zhang, Xi&apos;an JiaotongUniversity, China</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px" class="red-text">Prof Jon Barker,
									University of Sheffield, UK</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px" class="red-text">Prof. Hyung-Min
									Park, Sogang University, South Korea</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px" class="red-text">Professor Wenwu
									Wang, University of Surrey, UK</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px" class="red-text">Dr. Ziyi Yang,
									Microsoft
								</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px" class="red-text">Prof. Shigeo
									Morishima, Waseda University, Japan</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px" class="red-text">Dr. Peter Bell,
									Dr Lorena Aldana, University of Edinburgh, UK</span></li>
							<li style="margin-top: 5px;"><span style="font-size: 16px" class="red-text">Prof. Liu
									Xunying, The Chinese University of Hong Kong</span></li>
						</ul>
					</div>
					<div style="font-size: 16px;text-align:justify;">
						A timely review paper from the guest editors is also planned (provisional title: An Overview of
						Deep Neural Network Based Audio-Visual Speech Enhancement and Separation)
					</div>

					<div style="font-size: 16px;text-align:justify;margin-top: 20px;">
						H. Diversity of Guest Editors
					</div>
					<div style="font-size: 16px;text-align:justify; margin-top: 10px;">
						<ol>
							<li><span style="font-size: 16px">Technical diversity</span></li>
							<div style="font-size: 16px;text-align:justify; margin-bottom: 10px;">
								Guest editors include experts in multimodal speech signal processing, machine learning,
								information fusion, multimedia processing and hearing assistive technology.
							</div>
							<li><span style="font-size: 16px;">Geographical and gender
									diversity</span></li>
							<div style="font-size: 16px;text-align:justify;">
								The geographical distribution of the proposed guest editors includes Asia, Europe and
								North America. The Special Issue co-ordinating guest editors are from the UK and Taiwan.
								<span class="red-text">Two of our Guest Editors are females.</span>
							</div>
						</ol>
					</div>

					<div style="font-size: 16px;text-align:justify;margin-top: 20px;">
						I. Guest editors and their qualifications
					</div>
					<div style="font-size: 16px;text-align:justify; margin-top: 10px;">
						(Lead Guest Editor) AMIR HUSSAIN (Senior Member, IEEE) obtained his BEng (1st Class Honours) and
						PhD from the University of Strathclyde in Glasgow, UK, in 1992 and 1997 respectively. He is
						Director of the Centre of AI and Robotics at Edinburgh Napier University, UK. His research
						interests are cross-disciplinary and industry-led, and include a focus on cognitively-inspired
						multi-modal speech signal processing for assistive hearing and healthcare technologies. He has
						co-authored around 300 journal papers, supervised over 40 PhD students and led major national
						and international projects. He is currently leading research grants totalling over ?5M,
						including as Chief Investigator of the COG-MHEAR Programme Grant (funded under the UK EPSRC
						Transformative Healthcare Technologies 2050 Call) that aims to develop truly personalised,
						multi-modal hearing assistive technology. He is founding Chief Editor of (Springer's) Cognitive
						Computation journal and is/has been Associate Editor for various other journals including
						(Elsevier&apos;s) Information Fusion, the <i>IEEE Trans on Neural Networks and Learning Systems,
							IEEE Trans on Artificial Intelligence, IEEE Trans. on systems, Man Cybernetics: Systems, and
							IEEE Trans on Emerging Topics in Computational Intelligence. He is founding co-Chair of the
							UK Special Interest Group on Speech-based Multi-Modal Information Processing (UK-SIGMM),
							executive committee member of the UK Computing Research Committee (the national expert panel
							of the IET and BCS for UK computing research).</i> He has served as General Chair of IEEE
						WCCI 2020 (the world&apos;s largest technical event on computational intelligence, comprising
						the flagship IJCNN, FUZZ-IEEE and IEEE CEC) and the 2023 IEEE Smart World Congress (featuring
						six co-located IEEE Conferences).
					</div>
					<div style="font-size: 16px;text-align:justify; margin-top: 10px;">
						(co-Lead Guest Editor) YU TSAO (Senior Member, IEEE) received the B.S. and M.S. degrees in
						electrical engineering from National Taiwan University, Taipei, Taiwan, in 1999 and 2001,
						respectively, and the Ph.D. degree in electrical and computer engineering from the Georgia
						Institute of Technology, Atlanta, GA, USA, in 2008. From 2009 to 2011, he was a Researcher with
						the National Institute of Information and Communications Technology, Tokyo, Japan, where he
						engaged in research and product development in automatic speech recognition for multilingual
						speech-to-speech translation. He is currently a Research Fellow (Professor) and the Deputy
						Director with the Research Center for Information Technology Innovation, Academia Sinica,
						Taipei. He is also a Jointly Appointed Professor with the Department of Electrical Engineering,
						Chung Yuan Christian University, Taoyuan, Taiwan. His research interests include assistive oral
						communication technologies, audio coding, and bio-signal processing. He was the recipient of
						National Innovation Awards from 2018 to 2021, the Future Tech Breakthrough Award 2019, and the
						Outstanding Elite Award from the Chung Hwa Rotary Educational Foundation (2019-2020). His papere
						been awarded the 2021 IEEE Signal Processing Society (SPS), Young Author and Best Paper Awards.
						He is currently an Associate Editor for the <i> IEEE/ACM Transactions on Audio, Speech and
							Language Processing and IEEE Signal Processing Letters</i>.
					</div>
					<div style="font-size: 16px;text-align:justify; margin-top: 10px;">
						JOHN H.L. HANSEN (Fellow, IEEE) received Ph.D. & M.S. degrees from Georgia Institute of
						Technology, and B.S.E.E. degree from Rutgers Univ. He joined Univ. of Texas at Dallas (UTDallas)
						in 2005, where he is Associate Dean for Research, Professor of Electrical & Computer
						Engineering, Distinguished Univ. Chair in Telecommunications Engineering, and holds a joint
						appointment in School of Behavioral & Brain Sciences (Speech & Hearing). At UTDallas, he
						established the Center for Robust Speech Systems (CRSS). He is an ISCA Fellow, IEEE Fellow, past
						Member and TC-Chair of IEEE Signal Proc. Society, Speech & Language Proc. Tech. Comm.(SLTC), and
						Technical Advisor to U.S. Delegate for NATO (IST/TG-01). He served as ISCA President
						(2018-2021), and currently serves as Treasurer and ISCA Board Member. He has supervised 99
						PhD/MS thesis candidates (58 PhD, 41 MS/MA), was recipient of 2020 UT-Dallas Provost&apos;s
						Award for Graduate Research Mentoring, 2005 Univ. Colorado Teacher Recognition Award, and
						author/co-author of +865 journal/conference papers in the field of speech/language/hearing
						science, processing & technology with machine learning advancements. He served as General Chair
						for ISCA INTERSPEECH-2002, Co-Chair for ISCA INTERSPEECH-2022, Co-Organizer and Tech. Chair for
						IEEE ICASSP-2010, and Co-General Chair and Organizer for IEEE Workshop on Spoken Language
						Technology (SLT-2014) (Lake Tahoe, NV). He is serving as Tech. Chair for IEEE ICASSP-2024. He
						received the IEEE Signal Processing Society&apos;s Leo Beranek MERITORIOUS SERVICE AWARD in
						2021/22 "for exemplary service to and leadership in the Signal Processing Society."
					</div>
					<div style="font-size: 16px;text-align:justify; margin-top: 10px;">
						NAOMI HARTE is Professor in Speech Technology in the School of Engineering at Trinity College
						Dublin, Ireland. She is Co-PI and a founding member of the ADAPT SFI Centre. In ADAPT, she has
						led a major Research Theme centered on Multimodal Interaction involving researchers from
						Universities across Ireland and was instrumental in developing the future vision for the Centre
						for 2021-2026. She is also a lead academic of the hugely successful Sigmedia Research Group in
						the School of Engineering. She was appointed as an SFI Engineering Initiative Lecturer in
						Digital Media in TCD in 2008 Her research centres around Human Speech Communication. She treats
						speech as something we both hear and see, with a strong multimodal aspect to her work. Her
						research involves the design and application of mathematical algorithms to enhance or augment
						speech communication between humans and technology. Much of that work is underpinned by signal
						processing and machine learning, but also requires an understanding of how humans interact. Her
						current research projects include audio-visual speech recognition, speech synthesis evaluation,
						multimodal speech analysis, and birdsong. Her industrial background brings a real-world approach
						to her research. Prior to returning to academia, Naomi worked in high-tech start-ups in the
						field of DSP Systems Development, including her own company. She also previously worked in
						McMaster University in Canada. She was a Visiting Professor at ICSI in Berkeley in 2015, and
						became a Fellow of TCD in 2017. She earned a Google Faculty Award in 2018 and was shortlisted
						for the AI Ireland Awards in 2019. She currently serves on the Editorial Board of Computer
						Speech and Language and Chair of INTERSPEECH 2023.
					</div>
					<div style="font-size: 16px;text-align:justify; margin-top: 10px;">
						SHINJI WATANABE (Fellow, IEEE) is an Associate Professor at Carnegie Mellon University,
						Pittsburgh, PA. He has been actively working on deep learning based speech enhancement and
						separation for speech recognition applications, leading to the publication of more than 350
						papers in peer-reviewed journals and conferences and receiving several awards. These include the
						best paper award from the IEEE ASRU in 2019 for joint neural modeling of speech separation,
						beamforming, and speech recognition, which is related to this proposal. He also contributes to
						community-driven challenge activities, including CHiME speech recognition and separation
						challenges, which are famous speech processing activities, as an organizer and active
						participant. He organized two IEEE JSTSP Special issues on Self-Supervised Learning for Speech
						and Audio Processing and Far-Field Speech Processing in the Era of Deep Learning. He serves as a
						Senior Area Editor of the <i>IEEE Transactions on Audio Speech and Language Processing</i>. He
						was/has been a member of several technical committees, including the APSIPA Speech, Language,
						and Audio Technical Committee (SLA), IEEE Signal Processing Society Speech and Language
						Technical
						Committee (SLTC), and Machine Learning for Signal Processing Technical Committee (MLSP).
					</div>
					<div style="font-size: 16px;text-align:justify; margin-top: 10px;">
						ISABEL TRANCOSO is a full professor (retired), at Instituto Superior T&eacute;cnico (IST, Univ.
						Lisbon), the University where she got her PhD degree in 1987. She was the founder of the Human
						Language Technology Lab and the former President of the Scientific Council of INESC ID Lisbon.
						She chaired the ECE Department of IST, was Editor-in-Chief of the <i>IEEE Transactions on Speech
							and Audio Processing</i> and had many leadership roles in SPS (Signal Processing Society of
						IEEE) and ISCA (International Speech Communication Association), namely having been President of
						ISCA. She is vice-chair of the IEEE Fellow Committee. She was elevated to IEEE Fellow in 2011,
						and to ISCA Fellow in 2014.
					</div>
					<div style="font-size: 16px;text-align:justify; margin-top: 10px;">
						SHIXIONG ZHANG from Tencent AI Lab, USA. He received his Ph.D. degree from Cambridge University
						in 2014. From 2014 to 2018, he was a senior speech scientist at Microsoft, speech group.
						Currently he is a principal researcher at Tencent AI Lab leading the multi-modal research for
						speech recognition, speaker diarization, speech separation. He was granted the "IC Greatness
						award" by Microsoft in 2015 for his contribution on the "Personalised Hey Cortana" system in
						Windows 10. He was nominated a 2011 Interspeech Best Student Paper Award for his paper
						"Structured Support Vector Machines for Noise Robust Continuous Speech Recognition". Shi-Xiong
						has served as a Program Committee member of APSIPA and the Area Chair of several international
						conferences, including ICASSP, Interspeech and ASRU in 2021 and 2022. He is also a member of
						IEEE Signal Processing Society Speech and Language Technical Committee (SLTC).
					</div>

					<hr />
					<div style="font-size: 16px;text-align:justify;margin-top: 20px;">
						<b>Example related publications</b>
					</div>
					<div style="font-size: 16px;text-align:justify;  margin-top: 10px;">
						<ol style="list-style-type: none;">
							<li><span style="font-size: 16px">D. Michelsanti, Z.-H. Tan, S.-X. Zhang, Y.
									Xu, M. Yu, D. Yu, and J. Jesper, "An Overview of Deep-Learning-Based Audio-Visual
									Speech Enhancement and Separation," <i>IEEE/ACM Transactions on Audio, Speech, and
										Language Processing</i>, vol. 29, pp. 1368-1396, 2021.
								</span></li>
							<li><span style="font-size: 16px">S.-Y. Chuang, H.-M. Wang, and Y.
									Tsao, "Improved Lite Audio-Visual Speech Enhancement,"<i>IEEE/ACM Transactions on
										Audio, Speech and Language Processing</i>, vol. 30, pp. 1345-1359, 2022
								</span></li>
							<li><span style="font-size: 16px">J.-C. Hou, S.-S. Wang, Y.-H. Lai, Y. Tsao, H.-W. Chang,
									and H.-M. Wang, "Audio-visual Speech Enhancement using Multimodal Deep Convolutional
									Neural Networks," <i>IEEE Transactions on Emerging Topics in Computational
										Intelligence</i>, vol. 2(2), pp. 117-128, 2018.
								</span></li>
							<li><span style="font-size: 16px">C. Yu, K.-H. Hung, S.-S. Wang, Y. Tsao, and J.-w. Hung,
									"Time-Domain Multi-modal Bone/air Conducted Speech Enhancement,"" <i>IEEE Signal
										Processing Letters</i>, vol. 27, pp. 1035-1039, 2020. </span>
							</li>
							<li><span style="font-size: 16px">L. A. Passos, J. P. Papa., J. D. Ser. A. Hussain, and A.
									Adeel, "Multimodal Audio-visual Information Fusion using Canonical-correlated Graph
									Neural Networks for Energy-efficient Speech Enhancement," <i>Information Fusion</i>,
									vol. 90, pp. 1-11, 2023.
								</span></li>
							<li><span style="font-size: 16px">A. Adeel, M. Gogate, A. Hussain, and W. M. Whitmer,
									"Lip-reading Driven Deep Learning Approach for Speech Enhancement," <i>IEEE
										Transactions on Emerging Topics in Computational Intelligence</i>, vol. 5, no.
									3, pp. 481-490, 2021
								</span></li>
							<li><span style="font-size: 16px">M. Gogate, K. Dashtipour, and A. Hussain, "CochleaNet: A
									Robust Language-independent Audio-Visual Model for Speech Enhancement,"
									<i>Information Fusion</i>, vol. 63, pp. 273-285, 2020.
								</span></li>
							<li><span style="font-size: 16px">J. Reverdy, S. O&apos;Connor Russell, L. Duquenne, D.
									Garaialde, B. R. Cowan, and N. Harte. "RoomReader: A Multimodal Corpus of Online
									Multiparty Conversational Interactions,"" in Proc. <i>ELRA 2022</i>.
								</span></li>
							<li><span style="font-size: 16px">N. Harte and E. Gillen, "TCD-TIMIT: An Audio-Visual Corpus
									of Continuous Speech," <i>IEEE Transactions on Multimedia</i>, vol. 17, no. 5, pp.
									603-615, 2015.
								</span></li>
							<li><span style="font-size: 16px">T. Afouras, J. S. Chung, A., O. Vinyals, and A. Zisserman,
									"Deep Audio-visual Speech Recognition" <i>IEEE Transactions on Pattern Analysis and
										Machine Intelligence</i>, vol. 44(12), pp. 8717-8727, 2018.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">Z. Yu, Z. Yin, D. Zhou, D. Wang, F. Wong,
									and B. Wang,<i>"Talking Head Generation with Probabilistic Audio-to-visual Diffusion
										Priors</i>," in Proc. CVPR 2022.
								</span></li>
							<li><span style="font-size: 16px" class="red-text"> J. Richter, S. Frintrop, and T.
									Gerkmann. "Audio-visual Speech Enhancement with Score-Based Generative Models,"
									<i>arXiv:2306.01432</i>,2023.
								</span></li>
							<li><span style="font-size: 16px" class="red-text"> Q. Zhu, L. Zhou, Z. Zhang, S. Liu, B.
									Jiao, J. Zhang, et al., "Vatlm: Visual-audio-text Pre-training With Unified Masked
									Prediction for Speech Representation Learning,"<i>IEEE Transactions on
										Multimedia</i>.
								</span></li>
							<li><span style="font-size: 16px" class="red-text"> R. Tan, et al. "Language-Guided
									Audio-visual Source Separation via Trimodal Consistency," in <i>Proc. CVPR 2023</i>.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">I-C. Chen, et al. "Audio-visual Speech
									Enhancement and Separation by Utilizing Multi-modal Self-supervised Embeddings," in
									<i>Proc. ICASSPW</i>,2023.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">J.-C. Chou, C.-M. Chien, and K. Livescu,
									"AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised Features for
									Audio-Visual Speech Enhancement," <i>arXiv:2309.08030</i>, 2023.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">Y.-J. Lu, C.-Y. Chang, C. Yu, C.-F. Liu,
									J.-W. Hung, S. Watanabe, and Y. Tsao, "Improving Speech Enhancement Performance by
									Leveraging Contextual Broad Phonetic Class Information," <i>IEEE/ACM Transactions on
										Audio, Speech, and Language Processing</i>, vol. 31, pp. 2738-2750, 2023.
								</span></li>
							<li><span style="font-size: 16px" class="red-text"> Y. Li, and X. Zhang, "Lip Landmark-based
									Audio-visual Speech Enhancement With Multimodal Feature Fusion Network,"
									<i>Neurocomputing</i>, vol. 549, 2023.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">Xu, Haitao, et al. "A Multi-Scale Feature
									Aggregation Based Lightweight Network for Audio-Visual Speech Enhancement," in
									<i>Proc. ICASSP 2023</i>.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">M. Chu, Y. Ma, Z. Fan, M. Yang, Z. Tao,
									and D. Wu, "Gmasegan: A Global Multi-Head Attention Speech Enhancement Generative
									Adversarial Network," <i>SSRN 4395061</i>.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">L.-C. Chen, et al. "EPG2S: Speech
									Generation and Speech Enhancement Based on Electropalatography and Audio Signals
									using Multimodal Learning," <i>IEEE Signal Processing Letters</i>, vol. 29, pp.
									2582-2586, 2022.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">K.-C. Wang, et al. "EMGSE: Acoustic/EMG
									Fusion for Multimodal Speech Enhancement,"in <i>Proc. ICASSP 2022</i>.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">J. W. Hwang, J. Park, R. H. Park, and H.
									M. Park, "Audio-visual Speech Recognition Based on Joint Training with Audio-visual
									Speech Enhancement for Robust Speech Recognition," <i>Applied Acoustics</i>, vol.
									211, 2023.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">Z. Zhu, H. Yang, M. Tang, Z. Yang, S. E.
									Eskimez, and H. Wang, "Real-Time Audio-Visual End-To-End Speech Enhancement," in
									<i>Proc. ICASSP 2023</i>.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">T. Yoshinaga, K. Tanaka, and S.
									Morishima, "Audio-Visual Speech Enhancement with Selective Off-Screen Speech
									Extraction," <i>arXiv:2306.06495</i>.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">C. Valentini-Botinhao, A. L. A. Blanco,
									O. Klejch, and P. Bell, "Efficient Intelligibility Evaluation Using Keyword
									Spotting: A Study on Audio-Visual Speech Enhancement," in <i>Proc. ICASSP 2023</i>.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">L. A. Passos, et. al., "Multimodal Speech
									Enhancement using Burst Propagation," <i>arXiv:2209.03275</i>.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">K. Kinoshita, M. Delcroix, A. Ogawa, and
									T. Nakatani, "Text-informed Speech Enhancement with Deep Neural Networks," in
									<i>Proc. Interspeech 2015</i>.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">J. Wu et al., "Time Domain Audio Visual
									Speech Separation," in Proc. ASRU 2019J. Wu et al., "Time Domain Audio Visual
									Speech Separation," in Proc. ASRU<i> 2019.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">R. Gu, S.-X. Zhang, Y. Xu, L. Chen, Y.
									Zou, and D. Yu, "Multimodal Multi-channel Target Speech Separation," <i>IEEE Journal
										of Selected Topics in Signal Processing</i>, vol. 14, no. 3, pp. 530-541, 2020.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">Y.-L. Chien, H.-H. Chen, M.-C. Yen, S.-W.
									Tsai, H.-M. Wang, Y. Tsao, and T.-S. Chi, "Audio-Visual Mandarin Electrolaryngeal
									Speech Voice Conversion," in <i>Proc. Interspeech 2023</i>.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">C.-F. Liao, Y. Tsao, X. Lu, and H. Kawai,
									"Incorporating Symbolic Sequential Modeling for Speech Enhancement," in <i>Proc.
										Interspeech 2019</i>.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">H. Julien, J. Thomas, Z.
									V&eacute;ronique, and B. &Eacute;ric, "Configurable EBEN: Extreme Bandwidth
									Extension Network to Enhance Body-conducted Speech Capture," <i>IEEE/ACM
										Transactions on Audio, Speech, and Language Processing, in press.</i>
								</span></li>
							<li><span style="font-size: 16px" class="red-text">J. Chen, M. Wang, X. L. Zhang, Z. Huang,
									and S. Rahardja, "End-to-end Multi-modal Speech Recognition with Air and Bone
									Conducted Speech, in <i>Proc. ICASSP 2022</i>.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">H. Wang, X. Zhang, and D. Wang,
									"Attention-based Fusion for Bone-conducted and Air-conducted Speech Enhancement in
									the Complex Domain," in <i>Proc. ICASSP 2022</i>.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">H. Wang, X. Zhang, and D. Wang, "Fusing
									Bone-Conduction and Air-Conduction Sensors for Complex-Domain Speech
									Enhancement,"<i>IEEE/ACM Transactions on Audio, Speech, and Language Processing</i>,
									vol. 30,pp.3134-3143, 2022.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">Y.-W. Chen, K.-H. Hung, S.-Y. Chuang, J.
									Sherman, X. Lu, and Y. Tsao, "A study of Incorporating Articulatory Movement
									Information in Speech Enhancement," in <i>Proc. EUSIPCO 2021</i>.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">A. Ephrat, I. Mosseri, O. Lang, T. Dekel,
									K. Wilson, A. Hassidim, W. T. Freeman, and M. Rubinstein. "Looking to Listen at the
									Cocktail Party: A Speaker-independent Audio-visual Model for Speech Separation,"
									<i>ACM Transactions on Graphics</i>, vol. 37(4):111, 2018.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">A. Gabbay, A. Shamir, and S. Peleg,
									"Visual Speech Enhancement," in <i>Proc. Interspeech 2018</i>.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">A. Ephrat, I. Mosseri, O. Lang, T. Dekel,
									K. Wilson, A. Hassidim, W. T. Freeman, and M. Rubinstein. "Looking to Listen at the
									Cocktail Party: A Speaker-independent Audio-visual Model for Speech Separation,"
									<i>ACM Transactions on Graphics</i>, vol. 37(4):111, 2018.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">M. Liuzzolino and K. Koishida. "AV(se)2:
									Audio-visual Squeeze-excite Speech Enhancement," in <i> Proc. ICASSP 2020</i>.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">D. Michelsanti, Z.-H. Tan, S. Sigurdsson,
									and J. Jensen. "Deep-learning-based Audio-visual Speech Enhancement in the Presence
									of Lombard Effect," <i>Speech Communication</i>, vol. 115, pp. 38-50, 2019.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">M. Sadeghi, S. Leglaive, X.
									Alameda-Pineda, L. Girin, and R. Horaud, "Audio-visual Speech Enhancement using
									Conditional Variational Auto-encoders," <i>IEEE/ACM Transactions on Audio,
										Speech, and Language Processing</i>, vol. 28, pp.1788-1800, 2020.
								</span></li>
							<li><span style="font-size: 16px" class="red-text">Wu, Jian, et al. "Time domain audio
									visual speech separation." <i>2019 IEEE automatic speech recognition and
										understanding workshop (ASRU). IEEE</i>, 2019.
								</span></li>
							<li><span style="font-size: 16px" class="purple-text">Tan, Ke, et al. "Audio-visual speech
									separation and dereverberation with a two-stage multimodal network."<i>IEEE Journal
										of Selected Topics in Signal Processing 14.3 (2020)</i>: 542-553.
								</span></li>
							<li><span style="font-size: 16px" class="purple-text">A. Ephrat, I. Mosseri, O. Lang, T.
									Dekel, K. Wilson, A. Hassidim, W. T. Freeman, and M. Rubinstein. "Looking to Listen
									at the Cocktail Party: A Speaker-independent Audio-visual Model for Speech
									Separation," <i>ACM Transactions on Graphics</i>, vol. 37(4):111, 2018.
								</span></li>
						</ol>
					</div>

					<hr />
					<div style="font-size: 16px;text-align:justify;margin-top: 20px;">
						J. Tentative Dates
					</div>
					<div class="table-container">
						<table class="red-text">
							<tr>
								<td>Manuscript submission due:</td>
								<td style="padding: 0 20px;">April 30, 2024</td>
							</tr>
							<tr>
								<td>First review completed:</td>
								<td style="padding: 0 20px;">July 31, 2024</td>
							</tr>
							<tr>
								<td>Revised manuscript due:</td>
								<td style="padding: 0 20px;">October 15, 2024</td>
							</tr>
							<tr>
								<td>Second review completed:</td>
								<td style="padding: 0 20px;">January 31, 2025</td>
							</tr>
							<tr>
								<td>Final manuscript due:</td>
								<td style="padding: 0 20px;">Feb 28, 2025</td>
							</tr>
						</table>
					</div>

					<hr />

					<div style="font-size: 16px;text-align:justify;margin-top: 20px;" class="red-text">
						Draft Call For Papers for IEEE JSTSP Special Issue - this will be uploaded on website after
						approval:
					</div>
					<div style="font-size: 16px;text-align:justify; margin-top: 10px;">
						Voice is the most commonly used modality by humans to communicate and psychologically blend into
						society. Recent technological advances have triggered the development of various voice-related
						applications in the information and communications technology market. However, noise,
						reverberation, and interfering speech are detrimental for effective communications between
						humans and other humans or machines, leading to performance degradation of associated
						voice-enabled services. To address the formidable speech-in-noise challenge, a range of speech
						enhancement (SE) and speech separation (SS) techniques are normally employed as important
						front-end speech processing units to handle distortions in input signals in order to provide
						more intelligible speech for automatic speech recognition (ASR), synthesis and dialogue systems.
					</div>
					<div style="font-size: 16px;text-align:justify; margin-top: 10px;">
						Emerging advances in artificial intelligence (AI) and machine learning, particularly deep neural
						networks, have led to remarkable improvements in SE and SS based solutions. A growing number of
						researchers have explored various extensions of these methods by utilising a variety of
						modalities as auxiliary inputs to the main speech processing task to access additional
						information from heterogeneous signals. In particular, multi-modal SE and SS systems have been
						shown to deliver enhanced performance in challenging noisy environments by augmenting the
						conventional speech modality with complementary information from multi-sensory inputs, such as
						video, noise type, signal-to-noise ratio (SNR), bone-conducted speech (vibrations), speaker,
						text information, electromyography, and electromagnetic midsagittal articulometer (EMMA) data.
						Various integration schemes, including early and late fusions, cross-attention mechanisms, and
						self-supervised learning algorithms, have also been successfully explored.
					</div>
					<div style="font-size: 16px;text-align:justify; margin-top: 10px;">
						This timely special issue aims to collate latest advances in multi-modal SE and SS systems that
						exploit both conventional and unconventional modalities to further improve state-of-the-art
						performance in benchmark problems. We particularly welcome submissions for novel deep neural
						network based algorithms and architectures, including new feature processing methods for
						multimodal and cross-modal speech processing. We also encourage submissions that address
						practical issues related to multimodal data recording, energy-efficient system design and
						real-time low-latency solutions, such as for assistive hearing and speech communication
						applications.
					</div>
					<div style="font-size: 16px;text-align:justify; margin-top: 10px;">
						Special Issue research topics of interest relate to open problems needing addressed. These
						include, but are not limited to, the following.
					</div>
					<div class="row">
						<div class="col-xs-12">
							<ul style="margin-top: 10px;">
								<li><span style="font-size: 16px">Novel acoustic features and architectures
										for multi-modal SE (MM-SE) and multi-modal SS (MM-SS) solutions. </span></li>
								<li><span style="font-size: 16px">The integration of multiple data
										acquisition devices for multimodal learning and novel learning algorithms robust
										to imperfect data.
									</span></li>
								<li><span style="font-size: 16px">Few-shot/zero-shot learning and
										adaptation algorithms for MM-SE and MM-SS systems with a small amount of
										training and adaptation data.</span></li>
								<li><span style="font-size: 16px">Self-supervised and unsupervised learning
										techniques for MM-SE and MM-SS systems.
									</span></li>
								<li><span style="font-size: 16px">Approaches that effectively reduce model
										size and inference cost without reducing the speech quality and intelligibility
										of processed signals.</span></li>
								<li><span style="font-size: 16px">Novel objective functions that
										specifically aim to improve speech intelligibility/quality/automatic speech
										recognition accuracy.</span></li>
								<li><span style="font-size: 16px">Approaches that effectively reduce model
										size and inference cost without reducing the speech quality and intelligibility
										of processed signals.</span></li>
								<li><span style="font-size: 16px">Novel applications of MM-SE and MM-SS in
										human-human and human-machine communications. </span></li>
								<li><span style="font-size: 16px">Holistic evaluation metrics for MM-SE and
										MM-SS systems.</span></li>
								<li><span style="font-size: 16px" class="purple-text">(Large language
										model-based) Generative approaches for MM-SE and MM-SS.
									</span></li>
							</ul>
						</div>
					</div>

					<hr />

					<div style="font-size: 16px;text-align:justify;margin-top: 20px;">
						<b>Deadlines:</b>
					</div>
					<div class="table-container">
						<table class="red-text">
							<tr>
								<td>Manuscript submission due:</td>
								<td style="padding: 0 20px;">April 30, 2024</td>
							</tr>
							<tr>
								<td>First review completed:</td>
								<td style="padding: 0 20px;">July 31, 2024</td>
							</tr>
							<tr>
								<td>Revised manuscript due:</td>
								<td style="padding: 0 20px;">October 15, 2024</td>
							</tr>
							<tr>
								<td>Second review completed:</td>
								<td style="padding: 0 20px;">January 31, 2025</td>
							</tr>
							<tr>
								<td>Final manuscript due:</td>
								<td style="padding: 0 20px;">Feb 28, 2025</td>
							</tr>
						</table>
					</div>

					<hr />

					<div style="font-size: 16px;text-align:justify;margin-top: 20px;">
						<b>Guest Editors:</b>
					</div>
					<div class="table-container">
						<table>
							<tr>
								<td style="padding: 5px;">AMIR HUSSAIN (Edinburgh Napier University, UK)
								</td>
							</tr>
							<tr>
								<td style="padding: 5px;">YU TSAO (Academia Sinica, Taiwan)
								</td>
							</tr>
							<tr>
								<td style="padding: 5px;">JOHN H.L. HANSEN (University of Texas at Dallas, USA)
								</td>
							</tr>
							<tr>
								<td style="padding: 5px;">NAOMI HARTE (Trinity College Dublin, Ireland)
								</td>
							</tr>
							<tr>
								<td style="padding: 5px;">SHINJI WATANABE (Carnegie Mellon University, USA)</td>
							</tr>
							<tr>
								<td style="padding: 5px;">ISABEL TRANCOSO (Instituto Superior T&eacute;cnico, IST, Univ.
									Lisbon, Portugal)
								</td>
							</tr>
							<tr>
								<td style="padding: 5px;">SHIXIONG ZHANG (Tencent AI Lab, USA)
								</td>
							</tr>
						</table>
					</div>

				</div>
			</div>
</body>

</html>